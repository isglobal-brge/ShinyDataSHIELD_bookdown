[["index.html", "ShinyDataSHIELD Users Guide 1 Overview", " ShinyDataSHIELD Users Guide Escribà Montagut, Xavier; González, Juan R. 2021-07-19 1 Overview ShinyDataSHIELD is a non-disclosive data analysis toolbox powered by DataSHIELD with the following features: Descriptive statistics: Summary, scatter plots, histograms and heatmaps of table variables. Statistic models: GLM and GLMer model fittings Omic analysis: GWAS, LIMMA,  using different types of resources (VCF files, PLINK, RSE, eSets) The features available on ShinyDataSHIELD are powered by different packages of the DataSHIELD project (dsBaseClient and dsOmicsClient), it uses them in a seamless way so the final user of ShinyDataSHIELD can perform all the included studies without writing a single line of code and get all the resulting figures and tables by the click of a button. "],["setup.html", "2 Setup 2.1 R packages 2.2 Install", " 2 Setup In this section, a list of all the R packages required to launch ShinyDataSHIELD is given. Following it, two methodologies to install them is provided. The recommended methodology is to install all the packages using R and launch the Shiny app from there. However, depending on the version of R and other variables that cant be contemplated on a case to case basis, the user might have problems installing the R packages, if that is the case, an alternative has been created using Docker. 2.1 R packages The packages that contain the main functionalities of ShinyDataSHIELD are the following. DSI: The DataSHIELD Interface (DSI) handles the connections to the databases. DSOpal: DSOpal is an extension of DSI to connecto to Opal servers. dsBaseClient: Implementation of the base package R functions to obtain non-disclosive returns from the Opal servers (Example: Base package function as.factor is implemented as ds.asFactor). dsOmicsClient: Functions to perform non-disclosive omic analysis using BioConductor packages on the Opal servers. On the following table, all the packages required by ShinyDataSHIELD are shown along the versions used during development. Package Version DSI 1.1.0 DSOpal 1.1.0 dsBaseClient 6.0.1 dsOmicsClient 1.0.0 shinydashboard 0.7.1 shiny 1.4.0.2 shinyalert 1.1 DT 0.13 data.table 1.12.8 shinyjs 1.1 shinyBS 0.61 shinycssloaders 0.3 shinyWidgets 0.5.4 stringr 1.4.0 2.2 Install 2.2.1 Install required packages with R  Observation 1 The previous methodology has been tested on a clean installation of R 4.0.3 and RStudio 1.3, if any errors occur please consider using a clean install or refer to the official R documentation regarding the errors you are getting. Also, there is a known bug when using R 4.0.0 and Shiny, install R 4.0.3 or higher to solve it.  Observation 2 Please note that if you are using a Linux machine, the library libxml2 needs to be installed before trying to install the R packages, on a Ubuntu distribution it can be installed with sudo apt-get install libxml2, for other distributions please refer to the official documentation of it.  Observation 3 In order to install everything from RStudio, developer tools are needed. Since they are not downloaded by default with R or RStudio, please make sure to install them. On Windows machines Rtools needs to be installed (just install it, there is no need of performing the second step of the link Putting Rtools on the PATH), for Linux machines make sure the library make is installed, for Ubuntu distributions it can be installed by running sudo apt-get install build-essential on a bash terminal, for other distributions please refer to the official documentation of it. In order to install the packages, an R console is needed. The simplest way to get it is to install R and RStudio, launch RStudio and start typing commands to the R console. RStudio console To install the required packages input the following commands to the console: # Install devtools install.packages(&quot;devtools&quot;) # Install required packages to run ShinyDataSHIELD devtools::source_url(&quot;https://raw.githubusercontent.com/isglobal-brge/ShinyDataSHIELD/master/installer.R&quot;) # Install ShinyDataSHIELD devtools::install_github(&quot;isglobal-brge/ShinyDataSHIELD&quot;) The previous commands will install a library called devtools that is needed to source online scripts and run them. The next line sources a file that installs all the required packages to run ShinyDataSHIELD, finally the package that contains the Shiny application itself is installed. 2.2.1.1 Run the Shiny app Once all the packages are installed, ShinyDataSHIELD can be executed inputing the following command on the R console, which will launch the application on a new window. # Run ShinyDataSHIELD ShinyDataSHIELD::app() 2.2.2 Install with Docker Another option to use ShinyDataSHIELD is to install it using Docker. Docker can be installed on a Linux / Mac OS X machine without any complications as any other application, on Windows systems however it can be a little bit more troubling, there are many online resources to help. Please refer to the following links to install Docker on Windows Home, to setup the Linux Windows Subsystem and terminal and to execute Docker on Windows. Once Docker is up and running, execute the following command on a bash terminal (make sure Docker is running, if not search for the Docker Desktop app and launch it) to download and launch ShinyDataSHIELD. Be aware that the Docker images weights ~ 1.5 GB, so if your internet connection is slow it may take a while. docker run --rm -p 80:80 brgelab/shiny-data-shield The container will be exposed on the local port 80 and it will render on that port the application itself, so to start using ShinyDataSHIELD open your web browser of choice and go to the site localhost:80 At the beginning it may take some time for the application to render, this is because all the needed R libraries are being loaded, to be sure the container is actually working, take a look at the terminal where you inputed the Docker command, there you will see all the R verbose stating the libraries are being loaded. Once the user has finished using ShinyDataSHIELD, the container needs to be stopped to avoid wasting CPU resources, to do so, input the following command on a bash terminal (the command needs to be inputed on a new bash window): docker container ls This will prompt all the running containers, find the one with the NAMES brgelab/shiny-data-shield and copy its CONTAINER ID, then input the following bash command: docker stop xxxxxxxxxxxx Where xxxxxxxxxxxx is the CONTAINER ID. To run the application again, just enter the first bash command (docker run --rm -p 80:80 brgelab/shiny-data-shield), since it has already been downloaded, the application is cached on the computer and it will launch straight away. If the user wants to remove the Docker image from the computer, input the following bash command: docker image rm brgelab/exposome-shiny If the user wants to download the actual source code of the Shiny, install all the required packages and launch it locally on its machine, feel free to download it from Github. Theres a script called installer.R at the root of the repository with a short installer of all the required packages. Please note that the installer script may fail depending on the R version and others, for that reason is advised to always run the Docker version of ShinyDataSHIELD, as it only requires a single terminal command and will work no matter what. "],["functionalities.html", "3 Functionalities 3.1 Data entry 3.2 Table column types 3.3 Descriptive statistics 3.4 Statistic models 3.5 Genomics 3.6 Omics", " 3 Functionalities Along this section, an overview of the functionalities implemented on ShinyDataSHIELD is given. Theres information about how to use the funcionalities as well as some limitations or constraints to take into account when using ShinyDataSHIELD. 3.1 Data entry  DISCLAIMER Along this section the terms table and resource are widely used, it is important noting that when the autor talks about a table, it refers to what is shown as a table on the Opal server. A resource that holds a table is called (and treated) as a resource. The first step to any analysis is to load the required tables or resources to the study server(s). To do so, the user has to provide the server URL and the login credentials. This will allow the application to connect to the OPAL server and retrieve all the projects and resources / tables from them. Afterwards, the user can select the desired resources / tables and load them to the study servers. It is very important to understand the difference between a server (OPAL server) and a study server (R instances running inside an OPAL server), a study server can be visualized as an R session that has some tables and resources loaded. For that reason we can have multiple study servers on a single OPAL server, which is indeed needed when performing pooled studies. To properly explain how to load the desired study servers for multiple types of analysis, lets do a guided tour of the interface. Since Opal 4.2 there is a new concept introduced called profiles which correspond to different R servers (rockr). The user can choose which profile to use for each study server (more info here). Figure 3.1: Connect to server, initial state On Figure 3.1 the initial state of the Connect to server tab is shown, here we can see some credentials are already inputted, which correspond to a demo server. There is the option of performing a login to the server via traditional user/pass or using personal access token [PAT]. Figure 3.2: Connect to server, selecting tables and resources Once connected to the server, the interface is actualized showing the available projects that contain tables inside. To show the projects that contain resources, just click on the Table toggle. An example is illustrated on Figure 3.2. If we want to get further information from a table(s) or resource(s), select them and click on the Further information of selection, this will open the OPAL UI on a browser and will display the associated page for the selected items. If more than one item is selected, a page for each one will be opened. An example of this functionality is shown on the Figure 3.3 where a table was selected to retrieve further information. Figure 3.3: OPAL UI with information from a table Knowing which items are of our interest, we can add them to be loaded on the study servers, to do that select the items and click Add selected item(s). A new table will appear underneath showing everything selected, illustrated on Figure 3.4. On this table all the elements from all the servers will be added to have the general picture of what will be available on the study servers. Figure 3.4: Connect to server, selected items Reached this point, lets see different use cases and how we have to configure the application for them. 3.1.1 Single resource / table approach Not much to say about this, just select it and click the Connect button. Figure 3.5: Connect to server. Single resource / table approach 3.1.2 Multiple resource study Some studies require to put more than one resource on a single study server, this is the case of using VCF files to perform a GWAS; they require two resources, the VCF resource and the covariates resource (which is a resource that holds a plain table). For this use case, the user has to select the multiple resources from the dropdown inputs, add them to a single study and connect to it. Two important things must be said for this use case, 1) By default the tables/resources from one server are added to the same study server; 2) to have tables/resources on the same study server, they have to be from the same server, this translates to: we cant put a table from server X and a resource from server Y on the same study server. Figure 3.6: Connect to server. Single resource / table approach 3.1.3 Pooled data from the same server It is not uncommon that the same OPAL server has multiple tables that we wish to analyze using a pooled approach. To perform this kind of analysis we have to select all the tables (they have to consistent column-wise to be pooled). The important thing to take into account is that the tables need to be on different study servers in order to be analyzed using a pooled approach, in order to achieve that the user has to double click on the cells in order to edit them (Figure 3.7). There are a couple of rules regarding the naming of the study servers, 1) User inputted names cant be of the format StudyX (where X is a number), and 2) Similarly to what has been stated before, items from different servers cant be on the same study server. Figure 3.7: Connect to server. Study server edit Figure 3.8: Connect to server. Pooled data from the same server approach 3.1.4 Pooled data from different servers When pooling data from different servers, we have to separately login to all of them. On the upper part of the interface there is a + symbol used to add a new server (Figure 3.9), when clicking the - symbol the last server added will be removed from the interface. The procedure is exactly the same as what weve already seen, the only difference is that on the table of selected items, we will now see items from different servers (Figure 3.10). Since by default items from different servers are added to different study servers, there is no need to manually configure that. Figure 3.9: Connect to server. New server Figure 3.10: Connect to server. Pooled data from different servers approach 3.1.5 Study server profiles Extracted from the official documentation: A DataSHIELD profile is a R server profile combined with a DataSHIELD configuration (allowed functions, options and permissions). DataSHIELD users can then decide in which environment their analysis are to be performed, for a better reproducible science. On the opal-demo site, there is a different array of profiles, illustrated on the Figure 3.11. Figure 3.11: Opal demo profiles. Extracted from https://opaldoc.obiba.org/ In ShinyDataSHIELD, there is the option of assigning the required profile to each study server, there is only one requirement to be taken into account: A study server can only have one distinct profile. So the configuration table of Figure 3.12 is valid, and the configuration table of Figure 3.13 will yield a connection error (Figure 3.14). Figure 3.12: Correct profiles configuration Figure 3.13: Incorrect profiles configuration Figure 3.14: Incorrect profiles configuration error message 3.2 Table column types  TABLES USED TO DEMO THIS SECTION From https://opal-demo.obiba.org/ : STUDY TABLE PROFILE Study1a CNSIM.CNSIM1 default Study1b CNSIM.CNSIM2 default Study1c CNSIM.CNSIM3 default The table column types functionality is available for tables as well as the following resource types: SQL tables Tidy data files (tables): *.csv, *.tsv, etc All of the above options will be shown on the table that shows the available tables to use on this module. As stated on the use cases, to perform a pooled analysis the tables have to be on different study servers. Column integrity is checked before allowing the user the access to the other tabs. The function of this module is to get information about the class of each column of a table. Figure 3.15: Classes of the CNSIM table Aside from that, theres the option of changing the class of a column. This transformation is done using the proper DataSHIELD functions that perform disclosive checks before allowing the transformation. This is specially important for transformation from numeric to factor for example. The allowed classes to perform transformations are: Character Numeric Factor To perform a class change, double click on the desired row and and a drop-down menu will appear, choose the new class and click Confirm, after the checks, the table will be updated to display the new class. Figure 3.16: Class change Integer could be added if it was of interest. Please file an issue on GitHub if that is the case. 3.3 Descriptive statistics  TABLES USED TO DEMO THIS SECTION From https://opal-demo.obiba.org/ : STUDY TABLE PROFILE Study1a CNSIM.CNSIM1 default Study1b CNSIM.CNSIM2 default Study1c CNSIM.CNSIM3 default The descriptive statistics functionality is available for tables as well as the following resource types: SQL tables Tidy data files (tables): *.csv, *.tsv, etc All of the above options will be shown on the table that shows the available tables to use on this module. As stated on the use cases, to perform a pooled analysis the tables have to be on different study servers. Column integrity is checked before allowing the user the access to the other tabs. When using pooled data the descriptive statistics is by default of the pooled data, however, the graphical visualizations included on descriptive statistics provide the option of showing separated plots for the different studies. 3.3.1 Summary The summary provides non-disclosive insights on the different variables of the loaded data. This functionality is only available for factors and numeric variables, only variables that meet this criteria will be on the selector. When the desired summary is disclosive no table will be shown (as the function call returns an Error stating that the the return is disclosive). When the selected variable is a factor, the output shown is a count of all the different factors. It can be visualized with the pooled data or divided by study servers. When the selected variable is numerical, the output shown is a quantiles and mean table. It can be visualized with the pooled data or divided by study servers. 3.3.2 Scatter plot Create a non-disclosive scatter plot by selecting two numerical variables (one for each axis). This type of plot can only be generated using numerical variables, for that reason variables that do not meet this criteria are not shown on the selector. It can be visualized with the pooled data or divided by study servers. 3.3.3 Histogram Create a non-disclosive histogram of a selected variable. This type of plot can only be generated using numerical variables, for that reason variables that do not meet this criteria are not shown on the selector. It can be visualized with the pooled data or divided by study servers. 3.3.4 Heatmap Create a non-disclosive heatmap plot by selecting two numerical variables (one for each axis). This type of plot can only be generated using numerical variables, for that reason variables that do not meet this criteria are not shown on the selector. It can be visualized with the pooled data or divided by study servers. 3.3.5 Boxplot Create a non-disclosive Boxplot by selecting as many numerical variables as desired. This plot has the option of performing groupings using one or two factor variables from the same table. It can be visualized with the pooled data or divided by study servers. The Boxplot functionality uses ggplot2; a novel in-app editor named ggEditLite has been introduced inside the Shiny application to allow simple graphic modifications of the plot to adapt it to the style of choice, add title, subtitle, etc. This feature will be rolled out to all the plots as they get upgraded to make use of the ggplot2 package. 3.4 Statistic models  TABLES USED TO DEMO THIS SECTION From https://opal-demo.obiba.org/ : GLM and mixed models STUDY TABLE PROFILE Study1a CNSIM.CNSIM1 default Study1b CNSIM.CNSIM2 default Study1c CNSIM.CNSIM3 default Survival analysis STUDY TABLE PROFILE Study1a SURVIVAL.EXPAND_WITH_MISSING1 survival Study1b SURVIVAL.EXPAND_WITH_MISSING2 survival Study1c SURVIVAL.EXPAND_WITH_MISSING3 survival Statistic models are available for tables as well as the following resource types: SQL tables Tidy data files (tables): *.csv, *.tsv, etc There are three different statisticals models available to fit, GLM models (Statistics models tab), GLMer models (Mixed statistical models tab) and Survival cox models (Survival analysis tab). 3.4.1 GLM models The tab to fit a non-disclosive generalized linear models (GLM) contains a box to manually input the formula, a selector for the output family and a table displaying the variables of the data and the type of each variable. There is finally the option to perform a pooled analysis or a meta-study. The possible output families are: Gaussian Poisson Binomial Theres some help built into ShinyDataSHIELD regarding how to write the GLM formula, which is prompted to the user when clicking on the Formula input help button. The display of the variables can be toggled on and off for the convenience of use. Once the GLM model is fitted a table below the variables display will be rendered with the model results. The download button will prompt a system window to select where to store the shown table, it will save it as a *.csv. When using pooled data, the results of the GLM model will be of the combined data. (To do: Display more information of why a model fitment fails) When using a meta-study approach the results correpond to the betas and standard errors for three different meta study methodologies: Maximum Likelihood REstricted Maximum Likelihood Fixed-Effects meta-analysis This three methodologies can be visualized on a forest plot by selecting the desired one. 3.4.2 Mixed models The tab to fit non-disclosive generalized mixed effects models (GLMer) contains a box to manually input the formula, a selector for the output family and a table displaying the variables of the data and the type of each variable. The possible output families are: Poisson Binomial Theres some help built into ShinyDataSHIELD regarding how to write the GLMer formula, which is prompted to the user when clicking on the Formula input help button. The display of the variables can be toggled on and off for the convenience of use. Once the GLMer model is fitted a table below the variables display will be rendered displaying the results. The download button will prompt a system window to select where to store the shown table, it will save them as a *.csv. The mixed model results are independent for each study server. Theres a selector to toggle between the results of the different study servers. (To do: Display more information of why a model fitment fails) 3.4.3 Survival Analysis The tab to fit non-disclosive survival analysis is divided into four different subtabs. The first subtab, is used to create a survival object (See the information about survival::Surv function for more information https://www.rdocumentation.org/packages/survival/versions/2.11-4/topics/Surv). To create this object, three columns from the selected tables are needed and a parameter: (Information about each column copied from the survival::Surv function documentation) Start time variable: for right censored data, this is the follow up time. For interval data, the first argument is the starting time for the interval. End time variable: ending time of the interval for interval censored or counting process data only. Intervals are assumed to be open on the left and closed on the right, (start, end]. For counting process data, event indicates whether an event occurred at the end of the interval. Event variable: The status indicator, normally 0=alive, 1=dead. Other choices are TRUE/FALSE (TRUE = death) or 1/2 (2=death). For interval censored data, the status indicator is 0=right censored, 1=event at time (Start time variable), 2=left censored, 3=interval censored. For multiple enpoint data the event variable will be a factor, whose first level is treated as censoring. Although unusual, the event indicator can be omitted, in which case all subjects are assumed to have an event. Type of censoring (parameter): character string specifying the type of censoring. Possible values are right, left, counting, interval, interval2 or mstate. For the data used on this demo, the columns are the following: Start time variable: starttime End time variable: endtime Event variable: cens Type of censoring (parameter): counting We have to make sure all three columns are numeric on this demo: Once the survival object is created, the tab Fit survival model is unlocked. On this tab, as with the other statistic models, there is a help button and a Toggle variables tables to visualize the variables available to construct the model. The formula has the following structure: survival_object~tables_sm$variable+tables_sm$variable2 Where survival_object and tables_sm cant be modified. An example model for this demo data would be: survival_object~tables_sm$age+tables_sm$female If a model is fitted, the left two tabs can be accessed, this two tabs correspond to two possible visualization options of the survival analysis. The Meta analysis plots a forestplot of a selected model variable using different meta-analysis methods. The Visualization of model plots the survival curves for each study server. 3.5 Genomics  TABLES USED TO DEMO THIS SECTION From https://opal-demo.obiba.org/ : Analysis with BioConductor STUDY RESOURCE PROFILE Study1 RSRC.brge omics Study1 RSRC.brge_vcf omics Analysis with PLINK STUDY RESOURCE PROFILE Study1 RSRC.brge_plink omics Inside the genomics tab of dsOmicshiny there are two subtabs, one to perform analysis using BioConductor methods and another to perform analysis using PLINK methods. 3.5.1 Analysis with BioConductor To perform non-disclosive genomic analysis using BioConductor methodologies, the user has to input a VCF resource with a covariates resource (table) on the same study. When performing this kind of analysis, as explained on the Data Entry section, only one study server can be used. The Analysis with BioConductor has two sub-tabs, the first one corresponds to the GWAS, and as the name implies is used to perform a GWAS (Genome wide association study) non-disclosive analysis on the loaded data. There is a selector for the condition and the covariates to adjusted for. The fitted model is: snp ~ condition + covar1 +  + covarN. The results of the model appear on a table below the selectors. The download button will prompt a system window to select where to store the shown table, it will save it as a *.csv. The second subtab is to display a Manhattan plot of the GWAS results. The dowload plot button saves the shown figure as a *.png. 3.5.2 Analysis with PLINK To perform non-disclosive analysis using PLINK commands, the user has to load a SSH resource. The tab contains a field to input the PLINK command and a brief memo stating that when inputing the PLINK command to run there is no need of inputting it as plink ... as would be done on a terminal interface, the user has to input just the ...; also, there is no need to put out to indicate the output file. Once the command is run, a table with the results is displayed under the command input, the download button will prompt a system window to select where to store the shown table, it will save them as a *.csv. A button to display the raw terminal output appears to display the user on a popup the plain text. Theres also a sub-tab to show a Manhattan plot with the results obtained. The dowload plot button saves the shown figure as a *.png. 3.6 Omics  TABLES USED TO DEMO THIS SECTION From https://opal-demo.obiba.org/ : LIMMA STUDY RESOURCE PROFILE Study1 RSRC.GSE80970 omics DESeq and edge R: To be determined On the Omics tab there are three different subtabs for different methodologies to perform non-disclosive analysis: limma, DESeq and edgeR. The resources that can be used are ExpressionSets and RangegSummarizedExperiments. If the resources are pooled the user has to input each one in a different study on the data entry. 3.6.1 LIMMA The limma non-disclosive analysis tab contains two selectors to select the condition and covariables of the analysis (resulting formula is: feature ~ condition + covar1 +  + covarN), theres also a selector to input the annotations columns desired on the output of the analysis. Finally, theres a selector to indicate the type of data that is being studied, whether is microarray or RNAseq. Theres a selector to choose to do a surrogate variable analysis. Once the analysis is performed a table with the results is displayed below the parameter selectors. The download button will prompt a system window to select where to store the shown table, it will save them as a *.csv. If the analysis is being performed usging a pooled dataset, the shown table corresponds to all the pooled data. 3.6.2 DESeq To be implemented. 3.6.3 edgeR To be implemented. "],["developers-guide.html", "4 Developers Guide 4.1 File structure of ShinyDataSHIELD", " 4 Developers Guide Along this section, documentation for future developers and maintainers of ShinyDataSHIELD is provided. It contains information about how the whole Shiny application is structured, all the different scripts that contains, flowcharts of the different files and information on how to extend the capabilities of ShinyDataSHIELD to new types of resources as well as new methodologies.  Observation Please read this documentation with the actual source code on the side for easier understanding. 4.1 File structure of ShinyDataSHIELD Typically Shiny applications are contained in a single file or two files, since the typical structure of a Shiny application is to have a server function and a ui function that can be on the same file or split for larger applications. On ShinyDataSHIELD the server function has been split into different scripts where all of them contains the code of a certain block of the application. It has been done this way to not have a really long server file that is difficult to navigate and debug. There is no need to split the ui file into different scripts since it only contains the graphical declarations of the applications and is really easy to update and navigate. The different scripts that compose the whole ShinyDataSHIELD are the following: ui.R server.R, composed of the folowing scripts: connection.R descriptive_stats.R download_handlers.R genomics.R omics.R plot_renders.R statistic_models.R table_renders.R table_columns.R The file server.R exists to source the different files and it also includes some small funcionalities. Now a file per file explanation will be given with flowcharts (when needed), remarkable bits of code explanations and general remarks. Also, details on how to implement new functionalities will be given when needed. 4.1.1 ui.R Inside this file there are all the declarations of how the graphical user interface (GUI) will look like. First, it contains a declaration of all the libraries that have to be loaded for the application to run. The libraries are the following: DSI, DSOpal, dsBaseClient, dsOmicsClient, shinydashboard, shiny, shinyalert, DT, data.table, shinyjs, shinyBS, shinycssloaders, shinyWidgets, stringr). The next piece of code found jscode &lt;- &#39; $(document).keyup(function(event) { if ($(&quot;#password1&quot;).is(&quot;:focus&quot;) &amp;&amp; (event.keyCode == 13)) { $(&quot;#connect_server1&quot;).click(); }; if ($(&quot;#pat1&quot;).is(&quot;:focus&quot;) &amp;&amp; (event.keyCode == 13)) { $(&quot;#connect_server1&quot;).click(); } }); &#39; Is a JavaScript declaration that reads as: When the #password1 item (corresponds to the text input of the password on the data entry tab) is active (the user is writting in it) and the Intro key is pressed, trigger the #connect_server1 item (corresponds to the Connect button on the GUI). That provides the user the typical experience of inputting the login credentials and pressing Intro to log in. Its important noting that this is only the declaration of a string with the code inside, to actually make use of it, there is the line 58 of this same file that actually implements it. tags$head(tags$script(HTML(jscode))) Two more pieces of JavaScript and CSS are found jscode_tab &lt;- &quot; shinyjs.disableTab = function(name) { var tab = $(&#39;.nav li a[data-value=&#39; + name + &#39;]&#39;); tab.bind(&#39;click.tab&#39;, function(e) { e.preventDefault(); return false; }); tab.addClass(&#39;disabled&#39;); } shinyjs.enableTab = function(name) { var tab = $(&#39;.nav li a[data-value=&#39; + name + &#39;]&#39;); tab.unbind(&#39;click.tab&#39;); tab.removeClass(&#39;disabled&#39;); } &quot; css_tab &lt;- &quot; .nav li a.disabled { background-color: #aaa !important; color: #333 !important; cursor: not-allowed !important; border-color: #aaa !important; }&quot; The CSS is just for aesthetics, the JS however is to introduce the funcionality of enabling and disabling panels of the web application, this is used as js$disableTab() and js$enableTab() along the application. Those two scripts are integrated to the application when building the dashboardBody by using this useShinyjs(), extendShinyjs(text = jscode_tab, functions = c(&quot;enableTab&quot;, &quot;disableTab&quot;)), inlineCSS(css_tab) There are a some functions used in this file that are worth mentioning: hidden(): From the shinyjs library. The elements wrapped inside of this function will not be rendered by default, they have to be toggled from the server side. Example: A GUI element that needs to be displayed only when a certain condition is met. withSpinner(): From the shinycssloaders library. The elements wrapped inside of this function will be displayed as a loading spinner when they are being processed. This is used to wrap figure displays. Example: A plot that is being rendered, its better for the user experience to see a loading spinner so that it knows something is being processed rather than just staring at a blank screen waiting for something to happen. bsModal(): From the shinyBS library. Its used to prompt pop-ups to the user. Example: By the click of a button you want to render a pop-up to the application with a figure of an histogram of a selected column of a table. conditionalPanel(): From the shiny library. It is useful to display certain elements on the GUI regarding a condition is met or not, here is used to display the user / password fields or the personal access token (PAT) fields by checking the state of the selector. Note that the condition has to be written using JavaScript, thats why it looks like \"input.pat_switch1 == true\" rather than the typical R Shiny input$pat_switch1 == TRUE. In order to declare the elements when the user wants to add another server some R tricks are used, they are described and coded on the connection.R file. The rest of this file is your average Shiny functions and declarations, read the official documentation for any doubts. Please note that ShinyDataSHIELD uses shinydashboard to improve the looks and user experience, for any doubts regarding that please read its documentation. 4.1.2 server.R The server file is divided into the following blocks. Declaration of reactiveValues: As a code practice measure, all the variables that have to be used in different parts of the code (Example: Table that contains the information about the loaded resources, has to be written when loading the data and afterwards to check whether a resource has been loaded or not) are reactive values. The only occassions where there are regular variables are inside functions that use variables as placeholders to be used only inside of that function (Example: Storing the results of a middle ground operation to be later used inside the same function to perform the final analysis, whose results will be saved on a reactive value variable). Developers used to lower level languages can see this as public and private variables. Sourcing of scripts: Sourcing all the different scripts that actually make up server.R. As said before this is done this way to have a more structured application where each script takes care of a certain block of the application. Disabling of all the tabs except the server connector: By default all the tabs are visible on Shiny, in order to provide a good user experience all are disabled at the launch of the application using js$disableTab(), once tables or resources are loaded into the study servers tabs are enabled (only the ones that makes sense, if the user loads a Table, only the tabs to interacts with tables will be enabled). Function declaration: Declaration of a function that given a column of a data table will truncate the decimal places to 4, its used when rendering tables to not have tables with 9 decimals that look hideous. Functions to manage the Connected / No connection display. Its a bunch of logic and CSS to just control a small element of the GUI. Basically if the variable connection$active is TRUE the GUI will show Connected next to a green dot with a Disconnect button, otherwise it will display No connection next to a red dot. When the button Disconnect is pressed, the function to log out of the server is triggered and the connection$active variable is set to false. Stop function: This delcaration is left for future developers. When having trouble on a certain spot, add a stop button using actionButton(\"stop\", \"stop\"), press it on the GUI to stop the execution and perform the required debugging. The scripts sourced for by the server.R are the following: 4.1.2.1 connection.R This is probably the most important script of the whole application, as its the one that is responsible for loading the data in order to ensure that the application capabilities can be extended in the future painlessly (modular). Inside this script there are five different sections that are triggered by different actions: Creation of GUI for the new server tabs Creation of observeEvents for all the different server tab elements. URL builder to display selected items on a browser Connection to the server to obtain the projects and resources. Triggered by the button with label connect_server. Get tables / resources from the selected project. Triggered everytime the selector with label project_selected is changed. Add a study. Triggered by the button with label add_server. Remove a server tab. Triggered by the button with label remove. Remove a study item Triggered by the button with label remove_item. Load the selected studies to the study servers. Triggered by the button with label connect_selected. The first element to explain is the creation of the new server tabs (Point 1, observeEvent(input$add, {})). Its just a matter of noting two things to understand it easily, 1) The use of a reactive value tabIndex() which returns a integer (initialized at 1), this integer corresponds to the tab being created (hence its updated at the top of the call); 2) The rest of the call is an appendTab() that adds a new tab on the element id \"tabset1\" with the exact structure of ui.R but changing the element IDs using the reactive value so that all the buttons/input fields are numbered according to the tab they are located on. When removing a server tab (observeEvent(input$remove, {})) the tab itself is removed using removeTab() and the reactive value is actualized. The creation of observeEvents for all the different server tab elements (Subitems of point 2) is done using a small trick. max_servers number of observeEvents are created (using lapply) for the different functionalities so that all the server tabs are functional, this integer variable is defined on the script server.R, if more servers than the default (10) are required just update the definition of the variable and relaunch the application. The last part of the script, loads all the selected tables and resources to the selected study servers. It does everything needed to each particular type of resource, that means converting them to R objects or to tables depending on what they are. When loading the selected resources or tables into the study servers, the table available_tables is created. The name is a little bit confusing since it actually contains the information about tables and resources, the developer apologizes as this variable was set at the beginning of the development and has not been updated. Nevertheless, its an important variable of the application, the structure of this table is the following. Column Description name Name of the object (project.name) server_index Index of the study server that contains the table/resource server Name of the study server type_resource Type of the resource The Opal server can host different types of resources, to name a few there are ExpressionSet, RangedSummarizedExperiment and SQLResourceClient. Each type of resource needs a special treatment to be used, for example SQLResourceClient resources are plain tables, so they need to be converted to tables on the study server to use them. Currently the following resource types are supported by ShinyDataSHIELD. Resource type Treatment Name of the resource type on available_tables TidyFileResourceClient, SQLResourceClient - as.resource.data.frame(resource)- Append .t to the name table SshResourceClient Append .r to the name ssh GdsGenotypeReader - as.resource.object(resource) - Append .r to the name r_obj_vcf ExpressionSet - as.resource.object(resource)- Append .r to the name r_obj_eset RangedSummarizedExperiment - as.resource.object(resource)- Append .r to the name r_obj_rse Any other resource type - as.resource.object(resource)- Append .r to the name r_obj .r and .t are appended to the resources to allow a resource and a table on the same project to have the same names and not crash the Shiny application. Now, lets look at some examples to add new resource types on the connection.R file. There are different cases for the treatment that the new resource requires. Resources that just need to be loaded with no further action performed to them (same treatment as SSH connections). Add another else if statement after line 303. Example: New resource called Simple_resource else if (&quot;Simple_resource&quot; %in% resource_type){ # Update available_tables list with the new resource type name lists$available_tables &lt;- rbind(lists$available_tables, c(name = name, server_index = server_index, server = resources$study_server[i], type_resource = &quot;Simple_resource&quot;)) } Resources that need to be converted into R objects (datashield.assign.expr(conns, symbol = \"methy\", expr = quote(as.resource.object(res)))) and nothing else. Will work out of the box (the type_resource column of the lists$available_tables table will read r_obj). Resources that need to be converted into R objects (datashield.assign.expr(conns, symbol = \"methy\", expr = quote(as.resource.object(res)))) and be further processed. Add another else if statement after line 320. Example: A new type of resource called special_resource that contains some variable names that are desired to be saved on a variable to feed a list on the GUI. else if(&quot;special_resource&quot; %in% resource_type) { # Update available_tables list with the new resource type name lists$available_tables &lt;- rbind(lists$available_tables, c(name = name, server_index = server_index, server = resources$study_server[i], type_resource = &quot;special_resource&quot;)) # Perform the needed actions for this resource [...] } Finally, once all the connections have been successful, and all the selected tables and resources are loaded, the tabs that make use of the loaded objects are enabled by using (table examples) if(any(unique(lists$available_tables$type_resource) %in% c(&quot;table&quot;))) { show(selector = &quot;ul li:eq(2)&quot;) } There are many if that checks for type of resources and enables tabs if present, on the previous example the second tab ul li:eq(2) (there is no way of refering them by ID as far as I know to perform this action) is enabled because it contains a module that works with tables. If a new type of resource is implemented, add after line 353 (tab 10 is just as example) if(any(unique(lists$available_tables$type_resource) %in% c(&quot;new_resource&quot;))) { show(selector = &quot;ul li:eq(10)&quot;) } Also update this part if a new module is added, make sure to enable the tab only when the resources that the module use are present on the lists$available_tables. 4.1.3 Structure of the modules A common structure is followed for all the different modules, this refers to the general structure of descriptive_stats.R, statistics_models.R, genomics.R, omics.R and table_columns.R. Before describing the internal structure of the modules, lets briefly describe the GUI structure, which is also common between them. The tabs are filled with a tab box, the first element is always a table with the available tables / resources for that module. For example, the Omics module only displays the resources of type RSE or eSet. The other tabs are disabled by default and can only be accessed once the user has selected which resource to use. Now lets talk about how to accomplish all of this. At the beginning of all the modules there is an observeEvent that is triggered when the user selects an item from the table. The structure of this is the following observeEvent(input$table, { if(length(input$table_rows_selected) &gt; 0){ # Check if the user has selected any row different_study_server &lt;- TRUE # On this example we are checking that everything selected is on different study servers same_cols &lt;- TRUE # On this example we are checking tables to be pooled, so we are checking they have the same columns if(length(input$table_rows_selected) &gt; 1){ # If more than one table is selected the checks have to be performed, otherwise there is no need to check for same cols or different study servers same_cols &lt;- all(lapply(input$tqble_rows_selected, function(i){ res&lt;-all(match(lists$resource_variables[[as.character(lists$available_tables[type_resource %in% c(&quot;table&quot;)][i,1])]], lists$resource_variables[[as.character(lists$available_tables[type_resource %in% c(&quot;table&quot;)][1,1])]])) if(is.na(res)){FALSE} else{res} })) different_study_server &lt;- nrow(unique(lists$available_tables[input$table_rows_selected,3])) == length(input$table_rows_selected) } if(same_cols &amp; different_study_server){ # If both tests are OK, remove the &quot;resource_lim&quot; object from the study servers datashield.rm(connection$conns, &quot;resource_lim&quot;) for(i in input$table_rows_selected){ lists$available_tables[type_resource %in% c(&quot;table&quot;)][i,2] # Then assign the selected tables to a new variable on the study servers called &quot;resource_lim&quot;, this is the variable that all the other funcionalities of the module will refer to when performing analysis datashield.assign.expr(connection$conns[as.numeric(lists$available_tables[type_resource %in% c(&quot;table&quot;)][i,2])], &quot;resource_lim&quot;, as.symbol(as.character(lists$available_tables[type_resource %in% c(&quot;table&quot;)][i,1]))) } # Enable the analysis tab and update the GUI to display it js$enableTab(&quot;tab_of_analysis&quot;) updateTabsetPanel(session, &quot;id&quot;, selected = &quot;tab_of_analysis&quot;) } else{ # If the tests fail, display an error message shinyalert(&quot;Oops!&quot;, if(!same_cols){ &quot;Selected resources do not share the same columns, can&#39;t pool unequal resources&quot; }else{ &quot;Selected resources are not on different study servers, can&#39;t pool resources on the same study server.&quot; } , type = &quot;error&quot;) # Make sure analysis tabs are disabled and the GUI shows the selection tab, this is important to do because if the user first selects a valid table and then an invalid combination, we want to make sure that the user has no longer access to the analysis tab js$disableTab(&quot;tab_of_analysis&quot;) updateTabsetPanel(session, &quot;id&quot;, selected = &quot;table_selection&quot;) } } }) This example can be extended to the developers needs, but as a structure example is more than enough. Please read the source code for the available modules if extra examples are needed. The body of the modules correspond to whatever is needed on that module, let that be some observeEvent for buttons of the analysis tab, some renderUI for dynamic selectors or anything other that the module needs. The bottom of the modules is also shared, they contain an observe clause that is triggered when the tab is selected, it has the following structure observe({ if(input$tabs == &quot;id&quot;) { # The ID here corresponds to the tabname declare on the ui.R ; tabItem(tabName = &quot;ID&quot;, ....... tables_available &lt;- lists$available_tables[type_resource %in% c(&quot;table&quot;)] # Input here the type_resource that this module uses, so only those are displayed if(length(lists$resource_variables) == 0){ withProgress(message = &quot;Reading column names from available tables&quot;, value = 0, { for(i in 1:nrow(tables_available)){ # In this example we are reading table columns so that when the user selects from this table, we can automatically check if the columns are shared when trying to pool tables, this is done on the header of the module, that we have just seen lists$table_columns[[as.character(tables_available[i,1])]] &lt;- ds.colnames(as.character(tables_available[i,1]), datasources = connection$conns[as.numeric(tables_available[i,2])])[[1]] incProgress(i/nrow(tables_available)) } }) } # Finally we render the table with the available tables for this module so the user can select which ones to use, of course this needs to be completed on the table_renders.R (following chunk has an example) output$available_tables_sm &lt;- renderUI({ dataTableOutput(&quot;available_tables&quot;) }) } }) Example of the code for the table_render.R regarding the selection table output$available_tables &lt;- renderDT( lists$available_tables[type_resource == &quot;table&quot;], options=list(columnDefs = list(list(visible=FALSE, targets=c(0,2,4))), paging = FALSE, searching = FALSE) ) Now lets take a look at the scripts that are used by all the modules, their use is to render tables, figures and handle the downloads (figures + table downloads) 4.1.3.1 table_renders.R This script creates the displays of all the tables of ShinyDataSHIELD, it uses the DT package to do so. Besides the descriptive_summary table, all the other tables just render results from other functions. There are some things to point of this script: As can be seen in descriptive_summary table, you can actually perform operations inside of a renderDT function and display the result of them. The most used options for the tables aesthetics are the following options=list(columnDefs = list(list(visible=FALSE, targets=c(0))), paging = FALSE, searching = FALSE) This prevents the rownames column to be displayed (usually it just contains the numeration of rows 1N, be aware sometimes its of interest to see this column) and eliminates the paging and searching functionalities of the table. For small tables it makes sense to not show that but on big tables those options are set to TRUE, as its very useful to have a search box on them. The tables that display numerical columns (mixed or not with non-numerical columns) are actually passed through the format_num function (defined on server.R) so the displayed table has only four decimals but the actual table (the one that can be saved) has all the decimals. This is done using the following code as.data.table(lapply(as.data.table(vcf_results$result_table_gwas$server1), format_num)) This will pass each column to the function and if its numerical the decimals will be cut to 4. The table output structure of the LIMMA results look different than the others, this is because when performing a LIMMA with pooled resources it returns one table for each study, what is being done is just binding them to display to the user all the obtained results. There is a concrete render that needs a special mention on this documentation, that is the column_types_table, which uses the CellEdit JavaScript plugin to enable drop down menus when editing a table. Lets see what is being done tab &lt;- datatable( table_to_be_modified, editable = &quot;cell&quot;, callback = # The callback needs to be updated to include the JS custom code JS( &quot;function onUpdate(updatedCell, updatedRow, oldValue){&quot;, &quot;Shiny.onInputChange(&#39;jsValue&#39;, [updatedCell.index(), updatedCell.data()]);&quot;, # The results to actually update the table_to_be_modified on the module script will be retrieved by a observeEvent(input$jsValue, { ; change jsValue each time this approach is used to avoid collisions &quot;}&quot;, &quot;table.MakeCellsEditable({&quot;, &quot; onUpdate: onUpdate,&quot;, &quot; inputCss: &#39;my-input-class&#39;,&quot;, &quot; columns: [2],&quot;, &quot; confirmationButton: {&quot;, &quot; confirmCss: &#39;my-confirm-class&#39;,&quot;, &quot; cancelCss: &#39;my-cancel-class&#39;&quot;, &quot; },&quot;, &quot; inputTypes: [&quot;, &quot; {&quot;, &quot; column: 2,&quot;, &quot; type: &#39;list&#39;,&quot;, &quot; options: [&quot;, &quot; {value: &#39;numeric&#39;, display: &#39;numeric&#39;},&quot;, # Update this lines to declare the options of the dropdown &quot; {value: &#39;factor&#39;, display: &#39;factor&#39;},&quot;, &quot; {value: &#39;character&#39;, display: &#39;character&#39;}&quot;, &quot; ]&quot;, &quot; }&quot;, &quot; ]&quot;, &quot;});&quot;), options = list(pageLength = nrow(table_to_be_modified)) ) path &lt;- &quot;../../www/&quot; # folder containing the files dataTables.cellEdit.js # and dataTables.cellEdit.css, they are already included on ShinyDataSHIELD, so there is no need to worry about that dep &lt;- htmltools::htmlDependency( &quot;CellEdit&quot;, &quot;1.0.19&quot;, path, script = &quot;dataTables.cellEdit.js&quot;, stylesheet = &quot;dataTables.cellEdit.css&quot;) tab$dependencies &lt;- c(tab$dependencies, list(dep)) Example of what to include on the module script to update the table proxy = dataTableProxy(&#39;a&#39;) # No need to change this observeEvent(input$jsValue, { # As stated above, the trigger is actually the value defined on the callback, we can retrieve the row, column and value from that object change &lt;- data.table(input$jsValue) row &lt;- as.numeric(change[1,1]) + 1 column &lt;- as.numeric(change[2,1]) value &lt;- as.character(change[4,1]) table_to_be_modified[row, column] &lt;&lt;- value replaceData(proxy, table_to_be_modified, resetPaging = FALSE) } 4.1.3.2 plot_renders.R There are two types of plots on ShinyDataSHIELD, the ones created with the base function plot and the ones created with the ggplot library. In order to later recover the plots to download them, they actually have a different structure. Base plot structure: output$random_plot &lt;- renderPlot({ plots$random_plot &lt;- function(){ function_that_generates_the_plot_using_base_package(arguments) } plots$random_plot() }) For the base plots, a function is declared that returns the plot and is called to generate the plot to the GUI. Ggplot structure: output$manhattan &lt;- renderPlot({ plots$ggplot &lt;- function_that_generates_the_plot_using_ggplot2_package(arguments) plots$ggplot }) In this case the plot is saved, ggplot will generate a plot variable that can be called to render the plot. On this script there are two plots that are inside a renderCachedPlot function instead of a renderPlot because they take really long to calculate and its better to cache them. Inside of the renderPlot function some other code can be put, such as toggles to GUI elements or tryCatch() functions. 4.1.3.3 download_handlers.R In this script everything related to downloading plots and tables is found. There are basically three types of structures Table downloader: To download a *csv. Structure: output$table_download &lt;- downloadHandler( filename = &quot;table.csv&quot;, content = function(file) { write.csv( variable_that_contains_table , file, row.names = FALSE) } ) The row.names = FALSE argument may not be needed in tables where the row names are important. Base plot downloader: To download a *.png. Structure: output$base_plot_download &lt;- downloadHandler( filename = &quot;base_plot.png&quot;, content = function(file) { png(file = file) plots$base_plot() dev.off() } ) Basically this calls the previously declared function and captures the plot into a *.png. GGplot downloader: To download a *.png. Structure: output$ggplot_download &lt;- downloadHandler( filename = &quot;ggplot.png&quot;, content = function(file) { ggsave(file, plot = last_plot()) } ) When using ggplot, the function last_plot() renders the last plot rendered by ggplot. This only has one inconvenient, that is when you are downloading a plot that takes a while to render, the application doesnt show the save window dialog until it has rendered again. This should be addressed in the future as it really halters the user experience. 4.1.4 How to add a new block To add a new block to ShinyDataSHIELD, the developer has to create a new *.R script inside the inst/shinyApp/ folder of the project and give it a descriprive name of the function that it will perform. So the Shiny application actually sees it, the server.R needs to be updated and source the new file. Example: New block called new_analysis.R, the update to the server.R will be source(&quot;new_analysis.R&quot;, local = TRUE) Afterwards, the ui.R can be updated by defining how the new block will be presented to the user. The sidebarMenu function needs to be updated so that the new tab appears on the sidebar of the application, follow the structure of the other tabs. Afterwards update the dashboardBody function by defining all the different elements of the new tab, follow the structure of the other available tabs to follow the general design lines, all the functions that need to be used here are standard Shiny functions mostly and theres plenty of documentation and examples available online, when in doubt just try to copy an already implemented structure. Now the user can focus on the types of files that will feed this new block, if its a table theres no need to worry, if its a resource that is not implemented the connection.R needs to be updated. Read the above documentation for guidance on the changes that need to be done for new resources types. Once the GUI is setup and the table / resource that this block will use is setup, the backend for this block can be built on the new_analysis.R file. Include on that file all the required renderUI() functions and steps to process the file and analyze it. Probably a new variable will be required to hold the results, update the server.R header and include a new reactiveValues() declaration for the new block. If the new block requires to display tables or figures, update the table_renders.R and plot_renders.R following the given examples on their sections of the documentation. Make sure to include the download buttons for them on the download_handlers.R. If there is some part of the code that takes some time to process, theres the option of wrapping it inside the withProgress() function in order to display a loading annimation to the GUI to alert the user that something is being processed. Make sure to include the custom implementation of the header and footer functions for the module that have been presented before. When developing a new block there will probably be many problems occurring, in order to debug a Shiny application there is the browser() function, if the developer is getting some sort of error at X line of the script, just write browser() on the line adobe of the error, the execution will be stopped at that point and the developer can interact with all the available variables of the environment through the RStudio console, usually running the line that is giving an error on the console will provide enough information to kill the bug. If the line breaking is a function call it is advisable to type the variables that are being passed into the function on the console, that way the developer can see what exactly is being passed and can see that some argument is NULL when it shouldnt or its a character when it should be a number, those are quite common problems. When a new block is developed and integrated into ShinyDataSHIELD, please conclude it by updating this documentation and the user guide with a brief explanation of the new block and some remarks of the most interesting bits of it. "],["references.html", "References", " References "]]
